---
title: "CPLN 675: Interpolating Air Pollution Dynamics with Interpolation and Regression"
author: "Ken Steif & Michael Fichman"
date: "2/21/2022"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE,message = FALSE,cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen=999)
library(knitr)
```


# 1. Introduction

Weather, air pollution and elevation are typically represented as continuous spatial processes. However, they are often derived from discrete sample points like weather stations, air quality sensors and GPS units and *interpolated* into surfaces. This module teaches students how convert discrete samples of ozone to continuous spatial estimated ‘surfaces’. It also introduces regression-based geospatial predictive modeling.

The first part of the module contains instructions on how to conduct interpolation using kriging and inverse distance weighting procedures in ArcGIS and to calculate spatial lags.

The second part of the module involves running a regression model in R to estimate ozone levels.

## 1.2. Learning Objectives

- Understand the idea of spatial interpolation. Compare and contrast methodological approaches.

- Use the "fishnet" as a vector alternative to raster GIS.

- Introduction to geospatial predictive modeling: run some linear models and see how they work mechanically.

# 2. Data

Our operations in ArcGIS will use the `Mid_Atlantic_EPA_Dataset.shp` file. Open these data in ArcGIS and explore them. 

The `ozoneHigh` field is the highest measure of ozone sampled for 2016 at a given grid cell throughout the mid-Atlantic United States. Ozone is good in the stratosphere where it protects Earth from the Sun’s ultraviolet rays, but bad at ground-level because of its effects on respiratory health.

Grid cells denoted by `training == 1` are those areas where EPA maintains an ozone air quality sensor. The purpose of this first exercise is to predict ozone for all the locations where there are no sensors.

**Questions:**

What do you think should be our predictive strategy? 

Why is the spatial component so important here?

## 2.1. The Fishnet

These grid cells are a "fishnet" (which is an ESRI term) and we use them to approximate a raster approach to GIS while still having access to the tabular data that we like from vector GIS.

To create a fishnet in ArcGIS, there's a handy tool called "Create Fishnet" where you input a study area and specify a cell size. To create a fishnet in R (a skill you might want to apply on your midterm), consult the Appendix at the end of this markdown.

# 3. Interpolation in ArcGIS

The Arc module consists of the following steps:

- In ArcGIS - map `ozoneHigh` and some of the other other features in the data

- Create spatial lag of `ozoneHigh` in ArcGIS. 

- Create an IDW prediction in the `Mid_Atlantic_EPA_Dataset`.

- Create a Kriging prediction in the `Mid_Atlantic_EPA_Dataset`.

Start by visualizing `ozoneHigh` and seeing the spatial contours of the data.

## 3.1 Calculate a spatial lag in ArcGIS

Now that we’ve seen some more traditional interpolation techniques, let’s see what a regression can do for us.

We’re going to begin with some instructions on how to create the spatial lag variable in ArcGIS. This is the variable that will allow us to model the spatial autocorrelation. It is defined as the average ozoneHigh of any locations k nearest training neighbors.

1. Create a new shapefile of just the training grid cells, called `training`.

2. We are going to measure nearest neighbor distance from our fishnet cells to the training set. Using the ‘Generate Near Table’ tool, set the `Mid_Atlantic_EPA_Dataset` as the Input Features (make sure you have unselected any features you might have selected in the last step!) and use `training` as the Near Features. 

3. To specify how many nearest neighbors to measure distance to, uncheck ‘Find only closest feature’ and set ‘Maximum number of closest features to 3’. Call this new table `nearTraining`.

4. Note that the new table has the distance from each `IN_FID` from the Mid-Atlantic shapefile to its nearest `NEAR_FID` from the training shapefile. Note there are three entries for each `IN_FID` because we chose 3 nearest neighbors.

- We’re not interested in the distances per se - we want to get the average high ozone observation for each of the three nearest grid cells. We join the original `training` shapefile to the `nearTraining` using `NEAR_FID` from the latter to join to `OBJECTID` from the former (this might also be called FID, depending on your version of ArcGIS). Call this new table `nearTraining2`. Now we know the `ozoneHigh` of each training grid cell where there’s an air quality sensor.

5. Export this joined table to its own `.dbf` file or geodatabase table as `nearTraining2`.

6. We’re going to calculate the average `ozoneHigh` of each cell’s 3 nearest training neighbors. Add `nearTraining2` to your viewer; open the attribute table; right click on IN_FID and click `summarize`. Drop down the `ozoneHigh` field and make the statistic type `MEAN`. Name this table `nearTraining3` and run the tool.

7. Note that `nearTraining3` has 1,025 rows (one for each cell in the original Mid-Atlantic shapefile) and an average `ozoneHigh` calculation. If you have the wrong number of cells

8. Join `nearTraining3` to the original Mid-Atlantic shapefile and map the `average_OzoneHigh`.

9. Export this shapefile as `Mid_Atlantic_EPA_Dataset_withLag` Move this shapefile into R (see section 4.1 of this Markdown). If you can't complete this routine, you can catch up by using a similarly named data set found in the course github (the link to this data set, as a geojson, is below.)

## 3.2. IDW in ArcGIS

10. Use the `IDW` tool to create surface of estimated Ozone. Select `training` as your input features, and the `ozoneHigh` as the "Z-Value" field. Choose a raster cell size and extent that "make sense" for this analysis - it's your call as to what that is. You don't need to select a "Geostatistical Layer." Call this output `idw_1`

11. Use `Zonal Statistics as Table` to summarize `idw_1` by `Mid_Atlantic_EPA_Dataset` using the Zone Field `FID`. Choose statistics type "MEAN" and call the output `idw_zonal`.

12. Output this table as `idw_zonal.csv`.

## 3.3. Kriging in ArcGIS

13. Use the "Feature to Point" tool on `training` to get data for these sensors at point level. Call these points `kriging_centroid`.

14. Use the 'kriging' tool to create a surface. Input features are `kriging_centroid`, Z-Value Field is `ozoneHigh`. Set the cell size to be the same as `idw_1`. Let's leave the default parameters alone - do we really know anything as a class about the decay functions for Ozone?  Let's call the output `krige_1`

15. Use `Zonal Statistics as Table` to summarize `krige_1` by `Mid_Atlantic_EPA_Dataset` using the Zone Field `FID`. Choose statistics type "MEAN" and call the output `krige_zonal`.

16. Output this table as `krige_zonal.csv`.


# 4 Exploratory Analysis

We'll bring these data into R and explore them prior to creating a regression model.

## 4.1 Setup and Load Data

Let’s start by loading the requisite libraries and creating a `mapTheme`, reading the epa shapefile and plotting it using the native sf plotting function. Note that if you are new to the sf package you should go run-through this tutorial.

```{r libraries, message = FALSE, warning = FALSE}
library(tidyverse)
library(sf)
library(gridExtra)
library(viridis)
```

```{r mapTheme, echo=TRUE}
mapTheme <- theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  ) 
```

You can read in data in one of the following ways:

1. Put your output from the ArcGIS exercise above into the below code block:

```{r read_file, eval=FALSE}
epa <- st_read("myfilepath/myfile.shp") %>%
  st_transform(crs = 4326)
```


OR

2. Load this dataset from Github (which should match your data from the above process):

```{r read data, message = FALSE, warning = FALSE}
epa <- st_read("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/Mid_Atlantic_EPA_Dataset_withLag.geojson")
```

## 4.2 Build some plots

Let’s build some plots using ggplot. We begin with a map of ozone. How would you modify the below code block to get just the grid cells where `training = 1`? More info on colors here.

```{r}
ggplot() +
  geom_sf(data=epa , aes(fill=ozoneHigh)) + 
  scale_fill_viridis() +
  labs(title="Ozone concentration by grid cell, Mid-Atlantic region") +
  mapTheme
```

This map is strange. It’s representing grid cells for which there is no air quality sensor as having ozone of 0.000. That’s not quite correct, however. What is the next map doing differently?

```{r}
ggplot() +
  geom_sf(data=epa, fill="black", colour = NA) +
  geom_sf(data= filter(epa, training == 1), 
          aes(fill=ozoneHigh)) + 
  scale_fill_viridis() +
  labs(title="Ozone concentration by grid cell, Mid-Atlantic region") +
  mapTheme
```

# 5 Interpolating with regression

In this section we’re going to build a regression model to predict ozone as a function of a set of independent variables or ‘features’. We start with a bit of data cleaning and exploration.

The first code block is a bit tricky. Feel free to run it line by line to get a better sense of how it works. 

-`cbind` takes data frames of equal row length and combines them. Inside the `cbind` we have the following operations, separated by a comma:

- We pipe the `st_centroid` and `st_coordinates` commands to `epa` to create a data frame of X and Y coordinates to get some add reliable coordinates for our data.

- From the original `epa` layer we pull out just the variables we want for this analysis using `select`. Examine what we keep - some variables associated with the landscape and land use, and our Ozone lag `Ave_ozoneH`.

```{r cbind_epa2, warning = FALSE, message = FALSE}
epa2 <-
  cbind(epa %>% 
  st_centroid() %>% st_coordinates(),
    epa %>% 
    dplyr::select(HwyDensity,distWater,sumDevelop,sumForest,
           distI95,Population,distCities,training,
           ozoneHigh,Ave_ozoneH))
```


Let's create two more quintile maps like the ones below and grid.arrange them into one data visualization. Include a title.

Let’s then map two of these features using grid.arrange to splice them together.


```{r}
hwyPlot <- 
  ggplot() +
    geom_sf(data = epa2, aes(fill=HwyDensity)) + 
    scale_fill_viridis(name="Highway density") +
    mapTheme

developPlot <- 
  ggplot() +
    geom_sf(data=epa2, aes(fill=sumDevelop)) + 
    scale_fill_viridis(name="Developed area") +
    mapTheme

grid.arrange(hwyPlot,developPlot, ncol=1)
```


## 5.1 Setup

The goal is to predict ozoneHigh for every grid cell in our study area. However, we only have ozone data for a subset where training=1. Thus, the strategy is going to be to create a training set that we can use to train our model. When that model is robust, we will use it to predict for the entire dataset.

Step 1 is to pull out a training set, like so.
```{r}
training <- 
  epa2 %>%
  filter(training == 1)
```
Which variables do you think might correlate with ozoneHigh? Let’s create a small multiple plot.
```{r}
training %>% 
  dplyr::select(-training) %>%
  st_drop_geometry() %>%
  gather(Variable, Value, -ozoneHigh) %>%
    ggplot(aes(Value, ozoneHigh)) +
      geom_point() +
      geom_smooth(method = "lm", se=FALSE) +
      facet_wrap(~Variable, scales="free", ncol=5)
```
## 5.2 Creating Models

Let’s estimate the ‘kitchen sink’ regression with all of our variables **except** the spatial variables. Note the embedded use of `select` in the `lm` command where we remove the variables we don't want.

We take `training` and turn it from an `sf` to a data frame (to please the `lm` command, which wants a data frame), and we `select` to remove the variables we don't want.

Describe the summary and its goodness of fit - what are the significant variables? What are their coefficients and what does that mean? What is the r-squared value?

Overall, what do we make of this model?


```{r}
reg <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-Ave_ozoneH,-geometry,-training,-X,-Y))
summary(reg)

```

Let's estimate a new model, this time with x and y included as predictors. 

What do you notice? 

Why?


```{r}
reg2 <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-Ave_ozoneH,-geometry,-training))
summary(reg2)
```

## 5.3 Add the spatial lag

What is a spatial lag? 

What do you think about this correlation?


```{r}
ggplot(training, aes(Ave_ozoneH, ozoneHigh)) +
 geom_point() +
 geom_smooth(method="lm",se=F) +
 labs(title="Ozone as a function of the spatial lag of ozone")
```
Let’s include spatial lag in the regression

```{r}
reg.lag <- lm(ozoneHigh ~ ., data=training %>% 
                                   as.data.frame() %>%
                                   select(-geometry,-training))
summary(reg.lag)
```

What happened to all the variables? Why?


## 5.4 Asessing goodness of fit

R^2 is often the "go to" indicator of goodness of fit. Why? 

How do we interpret the R^2 statistic?

When we do predictive modeling however, R^2 can be misleading. A better way to think about error, when possible, is simply the difference between the observed value and the predicted value.

Let’s check out our predictions. First we create three new prediction fields and convert to long form - the format we need to create small multiple plots.

```{r}
training.summary <-
  training %>% 
    mutate(reg1.Pred = predict(reg, .),
           reg2.Pred = predict(reg2, .),
           reg.lag.Pred = predict(reg.lag, .)) %>%
    dplyr::select(c(ozoneHigh, starts_with("reg"))) %>%
    gather(Variable, Value, -ozoneHigh, -geometry) 

training.summary
```

Next, create three plots of predictions for observed. 

Which model looks most accurate?

```{r plot_training, message = FALSE, warning= FALSE}
ggplot(data = training.summary, aes(Value, ozoneHigh)) + 
  geom_point() + geom_smooth(method = "lm", se = FALSE) +
  facet_wrap(~Variable, scales = "free")
```

Finally, we calculate the Mean Absolute Error. absError is the absolute value of the difference between observed ozoneHigh and the predicted. The group_by allows us to calculate the average error by each regression type. Which model is the strongest?

```{r}
st_drop_geometry(training.summary) %>%
  mutate(absError = abs(ozoneHigh - Value)) %>%
  group_by(Variable) %>%
  summarize(MeanAbsoluteError = mean(absError))
```
Finally, let’s predict for the entire dataset.

This set of predictions is our interpolated output.

```{r}
epa2 <- mutate(epa2, reg.lag.pred = predict(reg.lag, epa2))

ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred)) + 
  scale_fill_viridis() +
  labs(title="Predicted ozone") +
  mapTheme
```
Let’s visualize the prediction with the areas where we have air sensors (ie. training == 1). 

We use the `predict` function to predict values for `epa2` using our model `reg.lag` and the independent variables in the `epa2` data set.

```{r}
epa2 <- epa2 %>% 
  mutate(reg.lag.pred = predict(reg.lag, epa2))

ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred)) + 
  geom_sf(data = st_centroid(filter(epa2, training == 1)), colour="black") +
  scale_fill_viridis() +
  labs(title="Predicted ozone") +
  mapTheme
```

# 6. Questions

What can you conclude about the statistical reliability of our predictions given this map? 

Were you able to conduct the IDW and Kriging maneuvers in ArcGIS? If so, how do these compare? Create a sequence of maps to examine the differences!

## 6.1. Bonus section - comparing methods

**If time allows, we will compare the outputs of our IDW and Kriging in ArcGIS to our lag and our regression predictions in R.**

Let's load in our kriging and IDW predictions that we output from ArcGIS. Here are your tasks:

1. Load your kriging and idw csv files using read.csv. Call these files `krige_pred` and `idw_pred`

```{r bonus_real, include=FALSE}
idw_pred <- read.csv("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/idw_zonal.csv")

krige_pred <- read.csv("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_6/data/wk6_airPollution/krige_zonal.csv")
```

2. Do some dplyr - `select` only columns `MEAN` and the unique fishnet cell ID from each table. Sometimes this unique ID is a little tricky - what's the right ID to get it to join to `epa`? `rename` the `MEAN` column to be `krige_pred` and `idw_pred` respectively.

```{r bonus_real_dplyr, include=FALSE}
idw_pred <- idw_pred %>%
  select(MEAN, FID) %>%
  rename(idw_pred = MEAN)

krige_pred <- krige_pred %>%
  select(MEAN, FID_) %>%
  rename(krige_pred = MEAN)
```

3. Use a tabular join

Join each data set to `epa` using the `left_join` function, like so:

```{r join_example, eval=FALSE}

new_data_set <- left_join(epa, idw_pred, by = c("left side unique ID" = "right side unique ID"))

```

```{r bonus_real_join, include=FALSE}
tables_joined<- idw_pred %>%
  left_join(., krige_pred, by = c("FID" = "FID_"))

epa_joined <- epa %>% left_join(., tables_joined, by = c("FID_1" = "FID"))
  
```

4. Can you map lag vs krige vs idw vs regression using ggplot and `grid_arrange`?

```{r ggplot_preds, echo=FALSE}
grid.arrange(ggplot() +
    geom_sf(data = epa_joined, aes(fill=Ave_ozoneH), color = "transparent") + 
    scale_fill_viridis(name="Spatial Lag") +
    mapTheme,
    ggplot() +
    geom_sf(data = epa_joined, aes(fill=idw_pred), color = "transparent") + 
    scale_fill_viridis(name="IDW") +
    mapTheme,
    ggplot() +
    geom_sf(data = epa_joined, aes(fill=krige_pred), color = "transparent") + 
    scale_fill_viridis(name="Kriging") +
    mapTheme,
    ggplot() +
  geom_sf(data=epa2, aes(fill = reg.lag.pred), color = "transparent") + 
  scale_fill_viridis(name="Regression") +
  mapTheme,
    ncol=2)
```

# Appendix 1 - Creating The Fishnet in R

The `sf` package offers really easy way to create fishnet grids - `st_make_grid`.

Let's load a spatial data file and make our very own. Remember our data from Chester County in Week 2? Let's load our Chester County municipalities file, then use `st_union` to go from a bunch of polygons to just one (e.g. the boundary).

```{r load_chesco, warning = FALSE, message = FALSE}
chesterBoundary <- read_sf("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_2/data/R_Data/Chester_MuniBoundaries.geojson") %>%
  st_union()

```

Not sure what the projection is? It's important to know - the cellsize you set for your fishnet will be in the native units of the projection. Check `st_crs(chesterBoundary)` to find out, and `st_transform` to the projection you need if necessary.

Here we are our CRS is 2272 (PA State Plane, linear unit = feet), so we're OK. We set the `cellsize = 10000` - 10,000 feet per cell.

Examine the fishnet - the unique ID is crucial to building a data set!

```{r make_fishnet}

fishnet <- 
  st_make_grid(chesterBoundary,
               cellsize = 10000, 
               square = TRUE) %>%
  .[chesterBoundary] %>%            # clips the grid to the chesterBoundary file
  st_sf() %>%
  mutate(uniqueID = rownames(.))
```


Voila, now you have a fishnet you can use for your analysis. Let's take a quick look at it and add the `chesterBoundary` for some context.

```{r plot_fishnet}

ggplot()+
  geom_sf(data = fishnet)+
  geom_sf(data = chesterBoundary, 
          color = "red", fill = "transparent")

```

# Appendix 2 - IDW and Kriging in R

Can you do Kriging or IDW in R? Of course you can!

Check out these resources to learn more:

Kriging is [usually done using an older spatial package called `sp`](https://towardsdatascience.com/building-kriging-models-in-r-b94d7c9750d8) but [here it's performed starting with an `sf` object.](https://gis.stackexchange.com/questions/287988/kriging-example-with-sf-object)
University of the Negev [has a bookdown](http://132.72.155.230:3838/r/spatial-interpolation-of-point-data.html) with spatial interpolation examples in R for both IDW and kriging.