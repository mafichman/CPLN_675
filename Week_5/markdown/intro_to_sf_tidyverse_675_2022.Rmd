---
title: "Mapping and Spatial Analysis in R"
author: "Michael Fichman"
date: "CPLN 675, Spring, 2022"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE, cache = TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
library(rmarkdown)
```

# Introduction

This tutorial will introduce you to the use of the `sf` package for vector-based spatial analysis and mapping. This code uses the `tigris` and `tidycensus` packages to query the US Census Bureau's databases of demographic and spatial information. It also integrates `sf` into the `ggplot2` data visualization workflow to create vector maps which visualize spatial information.

The "use case" for this exercise is a spatial analysis in which the locations and loads of toxic air polluting facilities are related to the nearby concentrations of school age children. 

How many children should Allegheny County (PA) schools be prepared to handle in the upcoming years who have been exposed to toxic air emissions? We will assume that there is some relationship between emissions exposure and learning disabilities school districts should be ready to allocate resources to districts with the greatest need for special education.

We want to know where all the toxic facilities are in Allegheny County and map them alongside a visualization of the percentage of pre-school aged children nearby. Additionally we want to know what school districts the facilities are in and how many children they can expect to have been exposed to emissions from age 0-5 during the years 2010-2015.

# Skills Learned in This Exercise

This exercise consists of a few classic planner's tasks:

1. Downloading spatial data from government APIs and open data sites

2. Importing and exporting shapefiles

3. Transforming data (e.g. changing its projection)

4. Creating and styling maps using `ggplot2` and `sf` with hydrology, labels and symbologies

5. Joining spatial data together by location

6. Summarizing point data by polygon geographies


# Resources

THere are lots of resources on the web you can consult to make `sf` and `ggplot2` work better for you. There are great ["cheat sheets"](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf) describing the capabilities of the sf package and [helping you style maps](https://www.r-spatial.org/r/2018/10/25/ggplot2-sf.html).

We are going to be doing reprojection of data when it suits us.   Each projection has an identifying number (a "crs" or coordinate reference system) which is listed at [spatialreference.org](http://spatialreference.org/).

# Setup 

## Install Libraries

If you haven't installed any of the folling packages, use the `install.packages` command to apply them as needed. You should have installed everything but `tigris` and `viridis` during Class 7.

(If you have them installed, you can skip this step)

```{r setup_packages1, warning = FALSE, eval = FALSE}
install.packages('tidyverse')
install.packages('tidycensus')
install.packages('sf')
install.packages('tigris')
install.packages('viridis')
```

Once the packages are installed, you must load them using the `library` command so that they are active in your environment.

```{r setup_packages2, warning = FALSE, message = FALSE}
library(tidyverse)
library(tidycensus)
library(sf)
library(tigris) # package to download tiger shapefiles from census API
library(viridis)
```

## Census API Key

You will need a "key" to access the Census API. You can find one at [their website](https://api.census.gov/data/key_signup.html).

Paste it into the code block below:

```{r load_key_hide, warning= FALSE, include=FALSE}
census_key <- read.table("~/GitHub/census_key.txt", quote="\"", comment.char="")
census_api_key(census_key[1] %>% as.character(), overwrite = TRUE)
```

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("YOUR KEY GOES HERE", overwrite = TRUE)
```

## Set Tigris Options

We want to set some options so that when we download data from the Census Bureau's Tigris spatial database, we get our data in `sf` format and we "cache" our downloads so they are stored on our machine.

```{r tigris_options, warning = FALSE, cache = TRUE}
options(tigris_class = "sf")
options(tigris_use_cache = TRUE)
```

## Set up color palettes, graphic styles, map style

The following code blocks are graphic "themes" we are going to use for our plots and maps. Instead of specifying sizes and fonts for each element of our graphics each time we want to make a graphic, we can store this information as an "object" and just add it to our "recipe" for each plot. Think of it as a global graphic style we are creating up front.

We set up `plotTheme` and `mapTheme` for our plots and maps, and create lists of colors called `palette` and `viridisPalette` which have hexidecimal color keys - color ramps.

Once you get a "recipe" that you like, you can use it in all of your scripts.

```{r graphic_themes, warning = FALSE, cache = TRUE}
plotTheme <- theme(
  plot.title =element_text(size=12),
  plot.subtitle = element_text(size=8),
  plot.caption = element_text(size = 6),
  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
  axis.text.y = element_text(size = 10),
  axis.title.y = element_text(size = 10),
  # Set the entire chart region to blank
  panel.background=element_blank(),
  plot.background=element_blank(),
  #panel.border=element_rect(colour="#F0F0F0"),
  # Format the grid
  panel.grid.major=element_line(colour="#D0D0D0",size=.75),
  axis.ticks=element_blank())

mapTheme <- theme(plot.title =element_text(size=12),
                  plot.subtitle = element_text(size=8),
                  plot.caption = element_text(size = 6),
                  axis.line=element_blank(),
                  axis.text.x=element_blank(),
                  axis.text.y=element_blank(),
                  axis.ticks=element_blank(),
                  axis.title.x=element_blank(),
                  axis.title.y=element_blank(),
                  panel.background=element_blank(),
                  panel.border=element_blank(),
                  panel.grid.major=element_line(colour = 'transparent'),
                  panel.grid.minor=element_blank(),
                  legend.direction = "vertical", 
                  legend.position = "right",
                  plot.margin = margin(1, 1, 1, 1, 'cm'),
                  legend.key.height = unit(1, "cm"), legend.key.width = unit(0.2, "cm"))

palette <- c("#10142A", "#47E9B9", "#F55D60", "#71EA48", "#C148EA", "#EAC148" )
viridisPalette <- c("#440154", "#73D055", "#F55D60", "#238A8D", "#FDE725")
```

# Loading Spatial Data With Tidycensus

## Load census data dictionaries

Now that we have our census credentials loaded, we can start downloading information from the API using some functions from tidycensus. We are going to make some comparisons between 2016 and 2010 ACS estimates for our target census tracts. In order to choose variables of interest, we are going to load the data dictionaries for each time period using `load_variables`.

What does this `load_variables` function do and why are we writing these things in the parentheses after the name of the function? If you want to know about a function in R, you can type the name of a package or function into the console like this: `??load_variables` and information will show up in the help window in your R Studio environment.

```{r load_variables, warning = FALSE, cache = TRUE}
acs_variable_list.2016 <- load_variables(2016, #year
                                         "acs5", #five year ACS estimates
                                         cache = TRUE)
```

Once we have loaded these data frames, we can observe and search through the data frames of variable information which should appear in our global environment either by clicking on them or using the `View(YOUR DATAFRAME NAME HERE)` command.

Let's look around in these data frames for a few minutes and see what's in there.

## Create a vector of census variables

We can populate a vector of variable names we will send to the Census API. We call this list `acs_vars`. This is the beauty of a code-based workflow - you can take this vector and put anything you want in it when you have a new analysis to do and re-run it for different variables. These need to be character strings, and hence, in quotes as you see below.

Keep in mind the categories and code numbers change a bit over time - you may need separate vectors for different census years.

Le create a vector called `acs_vars` which contains three census variables we are interested in - persons under 5 years old and a total population estimate.

```{r acs_vars, warning = FALSE, cache = TRUE}
acs_vars <- c("B01001_001E", # ACS total Pop estimate
              "B01001_003E", # Male:!!Under 5 years
              "B01001_027E") # Female:!!Under 5 years
```

## Call the Census API using tidycensus

We use the `get_acs` function in `tidycensus` to query the API. Notice the different arguments for the function, and that they require certain types of info. For example, `geography` requires one of a finite list of answers, and they have to be formatted as character string.

Remember the `??` function - you can learn about the parameters for `get_acs` this way. There is also a function called `get_decennial` which you can use for decennial census counts.

We ask for data on our `acs_vars` for all tracts in Allegheny County, PA in 2016. We ask for "wide" data (e.g. one variable per column, one row per tract) and we set `geometry` to `TRUE`.

```{r get_acsTracts.test, cache = TRUE, warning = FALSE, message = FALSE, results = "hide"}
acsTracts.2016 <- get_acs(geography = "tract",
                             year = 2016,
                             variables = acs_vars,
                             geometry = TRUE, # This is different from what we've been doing!
                             state = "PA",
                             county = "Allegheny",
                             output = "wide")
```

Let's examine these data using the `glimpse()` command to see how they look and what the data types are. 

```{r glimpse_acsTracts.2016, cache = TRUE}
glimpse(acsTracts.2016)
```

## Mutating, selecting and renaming variables

Now we can manipulate our data using some of our `tidyverse` data wrangling tools from the `dplyr` library.

The `dplyr` package is great for these operations and has some very common sense functions that are fairly intuitive because they use verbs. You can select columns (`select`), rename columns (`rename`), summarize data (`summarize`) by groups (`group_by`), doing arithmetic across columns (`mutate`).

The operator `%>%` is known as the "pipe" and lets you chain operations together - passing data along through different operations.

Let's manipulate our data using the "pipe" %>% and some tidy data wrangling commands. We will retain only certain variables by using the "select" command. We will rename some variables using "rename". We will create some new ones using "mutate"

Notice that we select only those variables inthe `acs_vars` list - we are only using the estimates from the ACS (suffix "E" on the variable names), we are not retaining the margin of error calculations (suffix "M" on the variable names).

```{r do_some_dplyr, warning = FALSE, cache = TRUE, results = "hide"}
acsTracts.2016 <- acsTracts.2016  %>%
  dplyr::select (GEOID, NAME, acs_vars) %>% # select just the GEOID and 
  rename (total.2016 = B01001_001E,
          male.under.5 = B01001_003E,
          female.under.5 = B01001_027E) %>%
  mutate(total.under.5 = male.under.5 + female.under.5,
         pct.under.5 = 100*(total.under.5/total.2016)) %>%
  st_as_sf() %>% # convert the shapefile to an "sf" object
  st_transform(crs=4326) # transform the data to the "web mercator" projection
```

## Converting to Simple Features

The `sf` package is a great way to deal with vector data - you can treat the data like a data frame for data wrangling operations, and map it using the powerful `ggplot` package. `sf` will recognize that the `geometry` column in our data is a set of instructions for drawing polygons.

We call the `st_as_sf` function to coerce these spatial data to `sf` format and `st_transform` to project them to the WGS84 Web Mercator coordinate system which is numbered `crs = 4326`.

```{r convert_to_sf, warning = FALSE, cache = TRUE}
acsTracts.2016 <- acsTracts.2016  %>%
  st_as_sf() %>% # convert the shapefile to an "sf" object
  st_transform(crs=4326) # transform the data to the "web mercator" projection
```


## Using dplyr and tidy syntax to code more efficiently

Let's try redo everything we just tried, only this time we will use a more efficient code routine, and do all the data intake at once. You can set up your own workflows like this for routines you expect to do repeatedly.

We are going to load and wrangle our data in one big series of functions chained together by the `%>%` operator. If you cant to get under the hood, you can just run portions of the call in your console window and see what the output looks like at each step.

We call the census API using `tidycensus` function `get_acs` for tract-level data representing our `acs_vars` for Allegheny County, PA using the 2016 five year estimates. 

We set our geometry to `TRUE` so we get a shapefile attached. 

We `select` only the GEOID, tract name and our `acs_vars` to remove our margins of error. We `rename` our variables to make them more intelligible.

We then `mutate` (e.g. create new variables) to tabulate the total population under 5 years old, both as a gross count and as a percentage.

Lastly we call the `st_as_sf` function to coerce these spatial data to `sf` format and `st_transform` to project them to the WGS84 Web Mercator coordinate system which is numbered `crs = 4326`.

```{r get_acsTracts.2016,  warning = FALSE, message = FALSE, results = "hide"}
acsTracts.2016 <- get_acs(geography = "tract",
                             year = 2016,
                             variables = acs_vars,
                             geometry = TRUE, # This is different from what we've been doing!
                             state = "PA",
                             county = "Allegheny",
                             output = "wide") %>%
  dplyr::select (GEOID, NAME, acs_vars) %>% # select just the GEOID and 
  rename (total.2016 = B01001_001E,
          male.under.5 = B01001_003E,
          female.under.5 = B01001_027E) %>%
  mutate(total.under.5 = male.under.5 + female.under.5,
         pct.under.5 = 100*(total.under.5/total.2016)) %>%
  st_as_sf() %>% # convert the shapefile to an "sf" object
  st_transform(crs=4326) # transform the data to the "web mercator" projection
```

We now have this census data with a geometry, and we can manipulate sf objects just like data frames. We can export them as shapefiles or remove the geometry and export them as csv files.

Use the `glimpse()` function to take a look at the data - you can do this in your console window.

# Mapping Polygons

Mapping polygons is easy with `ggplot2` and `sf` - we can just add a `geom_sf` to a ggplot!

```{r map1, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016)
```

We can get more and more complex from here. Let's add a `fill` aesthetic and visualize the `pct.under.5` by tract.

```{r map2, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5))
```

Let's keep adding on. We can make the `color` of our linework "transparent" to make things less busy.

And let's make the color ramp a bit nicer using the `viridis` package where we can send styling info to the `fill` command we have in our `aes` call.

```{r map3, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5), 
          color = "transparent")+
   scale_fill_viridis('Pct Under 5', # Scale label
                      direction = -1, # Direction of the color palette - either 1 or -1
                      option = 'D',  # Viridis has multiple palettes to choose from
                      alpha = 0.6) # Transparency level
```


# Tigris Data

Let's add some more data to our map using `tigris`, the package which calls the Census' shapefile repository. The Census Bureau keeps lots of data that isn't associated with the Census survey itself.

Let's load the county boundary shapefile using the `counties` function, transform it to an sf object (st_as_sf), st_transform it it to `crs=4326` (web mercator). Because we can only get counties a fill state at a time, we can then `filter` for just the county we are interested in.

If you want to see what the intermediate products are in this chain of functions, just run the first line or first two lines and then examine the object that's created to see how it's shaped step by step here.

```{r counties_tigris, message = FALSE, warning=FALSE, results = "hide"}
allegheny <- counties('PA') %>%
  st_as_sf()%>%
  st_transform(crs=4326)%>%
  filter(NAME == "Allegheny")
```

Let's add some water (for graphics funtimes) by getting another shapefile from tigris using a similar routine.

```{r area_water, warning = FALSE, message = FALSE, results = "hide"}
water <- area_water('PA', county = 'Allegheny') %>%
  st_as_sf()%>%
  st_transform(crs=4326)
```

# Spatial Joins

Let's load the unified school districts and get them set up as an `sf` object projected to `crs=4326`. We start with all the PA districts.

In order to keep only those districts which are within our county boundary shapefile, we do a spatial join using the `sf` function `st_join` - we only want districts that are within (`st_within`) our county boundary. We say we do not want to do a "left join" - in this join the state-wide districts are the left side of the join (functionally the first data file called) - we only want to keep what joins from the "right".

Note that when we do the join - the `st_join` function is looking for two files to join - since we "pipe" data into the funtion, the `.` stands in for the data being piped.

Why do you suppose this type of join gives us the output we want? Can you think of an alternative method for joining these data?


```{r unified_districts, cache = TRUE, warning = FALSE, message = FALSE, results = "hide"}
unified.districts <- school_districts('PA', type = "unified") %>%
  st_as_sf() %>%
  st_transform(crs=4326) %>%
  st_join(., allegheny, # do a join just keep those districts "within" the allegheny county boundary
          join = st_within, # this is the spatial join type from sf
          left = FALSE) # by specifying no "left" join - we keep only the allegheny county observations
```

Let's take a look at `unified.districts` using the `glimpse` command.

Notice that we have some ugly looking stuff in there - there are ".x" and ".y" appendages to everything that had an equivalent column name on either side of the join.

Let's clean up a bit and `select` the columns we want to keep and `rename` them.

We can also add columns for the lat/lon of the "centroid" points for each district which we can use to label the features later when we are making graphics.


```{r clean_unified_districts, cache = TRUE, warning = FALSE, message = FALSE}
 unified.districts <- unified.districts %>% 
  select(NAME.x, GEOID.x) %>% # let's just keep the variables we need - district name and geoid
  rename(NAME = NAME.x) %>% # both files had a "NAME" column, so the join turned one into "NAME.x" - let's rename it
  mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
         lat=map_dbl(geometry, ~st_centroid(.x)[[2]])) # add centroid values for labels
```

# More Mapping

Let's add the to our map and make the styling better. We can use our `mapTheme` aesthetics we specified earlier in the script. Let's change the options on `scale_fill_viridis` to keep our color ramp from conflicting with the blue water we're going to add.

Notice we are adding several `geom_sf` objects here and it's therefore important they be in the same coordinate system. We use WGS84 so much because we can use point data very easily which contains lat/lon (more on that later).

Let's also add some labels using the `labs` option.

```{r nice_map1, cache = TRUE}
ggplot()+
  geom_sf(data = acsTracts.2016, 
          aes(fill = pct.under.5), 
          color = "transparent")+
  scale_fill_viridis('Pct 0-5 year olds', # Scale label
                     direction = -1, # Direction of the color palette - either 1 or -1
                     option = 'D',  # Viridis has multiple palettes to choose from
                     alpha = 0.6)+ # Transparency level
  geom_sf(data = unified.districts,
          color = "white",
          fill= "transparent")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+ # let's make the boundary thick
  labs(
    title = "Distribution of pre-schoolers in Allegheny County by census tract",
    subtitle = "White polygons represent unified school districts",
    caption = "Data: US Census Bureau, 2016 ACS"
  )+
  mapTheme
```

If we wanted, we could output this map - either by using the `export` option in the Plot viewer or using `ggsave` - we can do this in pdf or eps file format and manipulate it in illustrator if we like.

# Loading Spatial Data from File

Let's go to WPC's open data site and download a csv of toxic release points in Allegheny County.

[https://data.wprdc.org/dataset/toxic-release-inventory/resource/b0c22ad4-7afb-496b-9500-3aebf91e67d6](https://data.wprdc.org/dataset/toxic-release-inventory/resource/b0c22ad4-7afb-496b-9500-3aebf91e67d6).

If you can't find it there, it's available at my github:

[https://raw.githubusercontent.com/mafichman/CPLN_675_wk5/main/data/toxic.csv](https://raw.githubusercontent.com/mafichman/CPLN_675_wk5/main/data/toxic.csv)

Then, read it in using either a line of code leading to the filepath (as below) or using the Import command in the "File" dropdown in R Studio

```{r load_toxic, cache = TRUE}
toxic <- read.csv("~/GitHub/CPLN_675/Week_5/data/toxic.csv")
```


Let's take a look at these using the `glimpse()` command.


Let's filter these data to just use toxic release events since 2010.


```{r filter_toxic, cache= TRUE}
toxic.5 <- toxic %>%
  filter(YEAR > 2010,
         COUNTY == "ALLEGHENY")
```

We don't need to turn this into an sf object at the moment. All our other data are projected in web mercator, which uses the same spatial reference as standard lat/lon values - so we can plot these using `geom_point` which is far faster to render. (You could go the `sf` route if you want, this is just for demonstration purposes.)


Let's visualize these points on the map alone. We can simply use geom_point if we are using unprojected, lat/lon data.

This is just like the last map but with points on top.

```{r nice_map2, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5, 
             aes(x=LONGITUDE, y=LATITUDE), 
             color = 'red')+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Probably not good that they are all by the rivers",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  mapTheme
```

# Summarizing Emission Data

We have 2047 observations but far fewer points, this means there are multiple releases per site. Let's summarize each site by number of releases and gross tonnage of released material. This requires us to `group_by` the `FACILITY_NAME` (and also the lat/lon so we can keep these pieces of info).

```{r toxic_5_summary, cache = TRUE, warning = FALSE, message = FALSE}
toxic.5.summary <- toxic.5 %>%
  mutate(obs = 1) %>% #add an "observation identifier - each release has an n of 1
  group_by(FACILITY_NAME, LATITUDE, LONGITUDE)%>%
  summarize(emissions.tons = sum(ON.SITE_RELEASE_TOTAL)/2000,
            n = sum(obs))
```

Let's make a plot with some graduated points to represent the volume of emissions


```{r nice_map3, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Probably not good that they are all by the rivers",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  mapTheme
```

# Making labels

Let's label the school districts (this ends up being kinda ugly)

```{r nice_map_labels1, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  geom_text(data=unified.districts, aes(x=lon, y=lat, label=NAME), alpha = 0.75, size = 2)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Subtitle",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  mapTheme
```

Let's try to do this again but remove the word "School District" from the NAME field. We can do this in our ggplot call and not even touch or alter the data set. Check out the text parsing function we can use from the `tidyverse` library `stringr` called `str_remove`.

```{r nice_map_labels2, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5,
          size = 1)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 1)+ # let's reduce the boundary thickness
  geom_point(data = toxic.5.summary, 
             aes(x=LONGITUDE, y=LATITUDE, size = emissions.tons), 
             color = 'red',
             alpha = 0.6)+
  geom_text(data=unified.districts, 
            aes(x=lon, y=lat, 
                label=NAME %>%
                      str_remove("School District")), 
            size = 2)+
  labs(
    title = "Toxic Release Points in Allegheny County, 2010-2015",
    subtitle = "Subtitle",
    caption = "Data: US Census Bureau, Allegheny County"
  )+
  mapTheme
```

# Spatial Joins - Points in Polygons

Now let's use spatial statistics to create summaries of both toxic emissions and demographics by school district.

First we will do a spatial join of points to polygons. We know we need to turn the `toxic.5.summary` dataframe into an sf object to make this work (`sf` spatial operations require `sf` objects).

We can do it *inside* the st_join function and give it the correct crs.

```{r st_join_points, cache = TRUE, warning = FALSE, message = FALSE}
districts.and.pollution <- st_join(unified.districts, toxic.5.summary %>%
                                     st_as_sf(crs=4326, 
                                              coords = c("LONGITUDE", "LATITUDE")))
```

Let's see what this looks like using the call `View(districts.and.pollution)`

Let's repeat the join we just did, but let's do some summary statistics - it's always nice to be efficient and chain functions together rather than throwing off intermediate dataframes and sf objects.

We group points by the name of the district (`NAME`) and summarize the total emissions and number of emission events per district (`sum(n)`). We use the `na.rm = TRUE` function to remove NA observations - districts with no emissions will trip up our summary calculations.

```{r st_join_districts2, cache = TRUE, warning = FALSE, message = FALSE}
districts.and.pollution <- st_join(unified.districts, toxic.5.summary %>%
                                  st_as_sf(crs=4326, 
                                               coords = c("LONGITUDE", "LATITUDE"))) %>%
  group_by(NAME) %>%
  summarize(emissions.tons = sum(emissions.tons, na.rm = TRUE),
            emissions.events = sum(n, na.rm = TRUE))
```

We can create a per-pupil emissions statistic to get more "normalized" measures. This means we have to join our tracts to our districts. We are going to do a centroid-based point-in-polygon join here - we want to avoid any modifiable aerial unit problems with our tracts. It turns out districts and tracts are NOT 100% coterminous. This is a complex exercise.

We calculate centroid points for our tracts in order to do this points-to-polygons join (`mutate` commands to create `lon` and `lat`). We then turn our `sf` into a data frame (`as.data.frame`) and strip out the `geometry` column. We then RECREATE an `sf` object using our centroid lat/lon pair as our geometry.

```{r point_in_poly2, cache = TRUE, warning= FALSE, message = FALSE }
acs.point <- acsTracts.2016 %>%
  mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
         lat=map_dbl(geometry, ~st_centroid(.x)[[2]])) %>%
  as.data.frame() %>%
  select( -geometry) %>%
  st_as_sf(crs=4326, 
           coords = c("lon", "lat"))
```

Explore these data using the `glimpse()` command. Make a simple `ggplot` to check them out.

```{r lil_ggplot, cache = TRUE}
ggplot()+
  geom_sf(data = unified.districts,
          color = "white",
          fill= "grey")+
  geom_sf(data = acs.point)
```

Now we can join points to polygons (tracts to districts) and summarize total population under 5 and the emissions per pupil by district.

```{r final_join, cache = TRUE, warning = FALSE, message = FALSE}  
districts.and.tracts <- districts.and.pollution %>%
  st_join(., acs.point) %>%
  group_by(NAME.x, emissions.tons, emissions.events) %>%
    summarize(total.under.5 = sum(total.under.5),
              total.pop = sum(total.2016),
              pct.under.5 = 100*(sum(total.under.5) / sum(total.2016))) %>%
  mutate(emissions.per.pupil = ifelse(is.na(emissions.tons) == FALSE, 
                                      (emissions.tons*2000) / total.under.5, 
                                      0))
```

# Mapping Final Analysis

Let's take a quick look at the distribution of our emissions-per-pupil calculation by making a histogram.

```{r quick_histogram, cache = TRUE, message = FALSE, warning = FALSE}
ggplot(data = districts.and.tracts, 
       mapping = aes(emissions.per.pupil))+
  geom_histogram(bins = 100)
```

There seems to be one big outlier, let's look at the lower end of the distribution (notice the `filter` command in the ggplot call)

```{r quick_histogram2, cache = TRUE, message = FALSE, warning = FALSE}
ggplot(data = districts.and.tracts %>%
         filter(emissions.per.pupil < 2000), 
       mapping = aes(emissions.per.pupil))+
  geom_histogram()
```


Symbologizing this data is best done using quantiles - they are intelligible and tend to be pretty even-handed in how they show the data. Let's add a quantile scale and use the `ntile` function inside our `aes` fill aesthetic to cut our data up right on the fly.

Another little trick here - we add labels using `geom_text` but filter out our highest emissions-per-pupil observations so we are only labelling hotspot areas.

```{r final_map, cache = TRUE, message = FALSE, warning = FALSE}
ggplot()+
  geom_sf(data = districts.and.tracts, 
          aes(fill = ntile(emissions.per.pupil, 4)))+
  scale_fill_viridis('Lbs per person - quintile breaks', # Scale label
                     direction = -1, # Direction of the color palette - either 1 or -1
                     option = 'D',  # Viridis has multiple palettes to choose from
                     alpha = 0.6,  # Transparency level
                     labels=as.character(quantile(districts.and.tracts$emissions.per.pupil,
                                                  c(.25,.5,.75,1))))+
  geom_sf(data = water, 
          color = 'blue', 
          alpha = 0.5)+ 
  geom_sf(data = allegheny, 
          color = "black", 
          fill = 'transparent', 
          size = 2)+ # let's make the boundary thick
  labs(
    title = "Toxic emissions per persons 0-5 years old, 2010-2015, by school district",
    subtitle = "Some districts have zero emissions, a few have a very large volume. Top 10 districts are labelled.",
    caption = "Data: US Census Bureau, 2016 ACS, State of PA"
  )+
  geom_text(data= districts.and.tracts %>%
              mutate(lon=map_dbl(geometry, ~st_centroid(.x)[[1]]), # add centroid values for labels
                     lat=map_dbl(geometry, ~st_centroid(.x)[[2]])) %>%
              ungroup()%>%
              arrange(-emissions.per.pupil )  %>% 
           slice(1:10), 
            aes(x=lon, y=lat, 
                label=NAME.x %>%
                str_remove("School District")), 
            color = "white",
            size = 2)+
  mapTheme

```

# Exporting and importing shapefiles

You can write out any sf objects as .shp files which are readable in ArcGIS. This is especially useful if you want to just use R to grab data efficiently and want to map it elsewhere.

Use the fuction "st_write"

`??st_write`

You can also read in shapefiles with an `shp` file extension using `st_read`.

# Bonus - Spatial Data with mapview

The `mapview` package is a handy tool for making interactive maps with `sf`.

If you don't have the package installed, install it with `install.packages("mapview")` prior to running the code below:

```{r load_mapview, warning = FALSE, message = FALSE}
library(mapview)
```

```{r mapview_emissions}
mapView(districts.and.tracts, zcol = "emissions.per.pupil")
```